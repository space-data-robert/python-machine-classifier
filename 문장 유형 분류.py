# -*- coding: utf-8 -*-
"""문장유형분류.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PrNsLl0SazDpFvB0aEutbMdSEnvYLuKW

- 드라이브 접근
"""

from google.colab import drive
drive.mount('/content/drive')

data_nm: str = '/content/drive/MyDrive/sentence.csv'

"""- 데이터셋 로드"""

import warnings
import pandas as pd
warnings.filterwarnings('ignore')

data = pd.read_csv(data_nm)
print(f'data.shape = {data.shape}')
data.tail(2)

train_len = len(data) * 0.8

train = data.loc[:train_len]
test = data.loc[train_len:]
print(f'train.shape = {train.shape}')
train.tail(2)

"""- 데이터셋 객체"""

import torch

class SentenceDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, ind):
        item = {key: torch.tensor(val[ind]) for key, val in self.encodings.items()}

        if self.labels:
            sentence_type = self.labels['type'][ind]
            sentence_polarity = self.labels['polarity'][ind]
            sentence_tense = self.labels['tense'][ind]
            sentence_certainty = self.labels['certainty'][ind]
            
            item['labels'] = (
                torch.tensor(sentence_type), 
                torch.tensor(sentence_polarity), 
                torch.tensor(sentence_tense), 
                torch.tensor(sentence_certainty)
            )
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

"""- 허깅페이스 모델"""

!pip install transformers

from transformers import Trainer, TrainingArguments

class SentenceTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop('labels').to(torch.int64)
        
        type_logit, polarity_logit, tense_logit, certainty_logit = model(**inputs)
        
        criterion = {
            'type' : FocalLoss().to(device),
            'polarity' : FocalLoss().to(device),
            'tense' : FocalLoss().to(device),
            'certainty' : FocalLoss().to(device)
        }

        loss = (
            criterion['type'](type_logit, labels[::, 0]) + 
            criterion['valence'](polarity_logit, labels[::, 1]) + 
            criterion['tense'](tense_logit, labels[::, 2]) +
            criterion['certainty'](certainty_logit, labels[::, 3])
        )

        outputs = (
            None, 
            torch.argmax(type_logit, dim=1),
            torch.argmax(polarity_logit, dim=1),
            torch.argmax(tense_logit, dim=1),
            torch.argmax(certainty_logit, dim=1)
        )
        return (loss, outputs) if return_outputs else loss

"""- 데이터 전처리"""

from sklearn.preprocessing import LabelEncoder

encoder_type = LabelEncoder()
encoder_type.fit(data['type'])

encoder_polarity = LabelEncoder()
encoder_polarity.fit(data['polarity'])

encoder_tense = LabelEncoder()
encoder_tense.fit(data['tense'])

encoder_certainty = LabelEncoder()
encoder_certainty.fit(data['certainty'])

def encoding(x_train, x_valid):
    x_train['type'] = encoder_type.transform(x_train['type'])
    x_valid['type'] = encoder_type.transform(x_valid['type'])

    x_train['polarity'] = encoder_polarity.transform(x_train['polarity'])
    x_valid['polarity'] = encoder_polarity.transform(x_valid['polarity'])

    x_train['tense'] = encoder_tense.transform(x_train['tense'])
    x_valid['tense'] = encoder_tense.transform(x_valid['tense'])

    x_train['certainty'] = encoder_certainty.transform(x_train['certainty'])
    x_valid['certainty'] = encoder_certainty.transform(x_valid['certainty'])

    train_labels = {
        'type' : x_train['type'].values,
        'polarity' : x_train['polarity'].values,
        'tense' : x_train['tense'].values,
        'certainty' : x_train['certainty'].values
    }

    valid_labels = {
        'type' : x_valid['type'].values,
        'polarity' : x_valid['polarity'].values,
        'tense' : x_valid['tense'].values,
        'certainty' : x_valid['certainty'].values
    }
    
    return train_labels, valid_labels

from transformers import AutoTokenizer, AutoConfig # AutoModelForSequenceClassification, EarlyStoppingCallback, AutoModel, 

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_path = 'beomi/KcELECTRA-base-v2022'

tokenizer = AutoTokenizer.from_pretrained(model_path)
length = data['sentence'].str.len().max()

config = AutoConfig.from_pretrained(model_path)
config.name_or_path = 'admin'

from torch import nn
from transformers import AutoModel

class SentenceModel(nn.Module):
    def __init__(self):
        super(SentenceModel, self).__init__()
        if model_path == 'monologg/kobigbird-bert-base':
            config.attention_type = 'original_full'
        
        self.base_model = AutoModel.from_pretrained(model_path, config=config)
        try:
            self.out = self.based_model.encoder.layer[-1].output.dense.out_features
        except:
            self.out = 768
    
        self.type_classifier = nn.Sequential(
            nn.Dropout(p=0.2),
            nn.Linear(in_features=self.out, out_features=4),
        )
        self.polarity_classifier = nn.Sequential(
            nn.Dropout(p=0.2),
            nn.Linear(in_features=self.out, out_features=3),
        )
        self.tense_classifier = nn.Sequential(
            nn.Dropout(p=0.2),
            nn.Linear(in_features=self.out, out_features=3),
        )
        self.certainty_classifier = nn.Sequential(
            nn.Dropout(p=0.2),
            nn.Linear(in_features=self.out, out_features=2),
        )

    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):
        x = self.base_model(
            input_ids=input_ids, 
            attention_mask=attention_mask
        )[0]

        output_type = self.type_classifier(
            x[:,0,:].view(-1,self.out)
        )
        output_polarity = self.polarity_classifier(
            x[:,0,:].view(-1,self.out)
        )
        output_tense = self.tense_classifier(
            x[:,0,:].view(-1,self.out)
        )
        output_certainty = self.certainty_classifier(
            x[:,0,:].view(-1,self.out)
        )
        return output_type, output_polarity, output_tense, output_certainty

import gc

gc.collect()
torch.cuda.empty_cache()

tokenized = tokenizer(
    test['sentence'].tolist(), 
    padding=True, truncation=True, 
    max_length=length, return_tensors='pt'
)

test_dataset = SentenceDataset(tokenized, None)
test_args = TrainingArguments(
    output_dir='./',
    do_train=False,
    do_predict=True,
    per_device_eval_batch_size=256,   
    dataloader_drop_last=False    
)

test_results = []

with torch.no_grad():
    for i in range(5):
        model = SentenceModel().to(device)
        
        trainer = SentenceTrainer(model=model, args=test_args)

        test_results.append(
            trainer.predict(test_dataset)
        )
        del model
        del trainer
        
        gc.collect()
        torch.cuda.empty_cache()

"""- 참고자료 [링크](https://dacon.io/competitions/official/236037/codeshare/7335?page=1&dtype=recent)

"""